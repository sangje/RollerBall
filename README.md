# PPOλ¥Ό μ΄μ©ν• RollerBall κ²μ„ ν•™μµ ν”„λ΅μ νΈ

## π® ν”„λ΅μ νΈ μ†κ°

μ΄ ν”„λ΅μ νΈλ” Unity ML-Agentsλ¥Ό μ‚¬μ©ν•μ—¬ κ°•ν™”ν•™μµ μ•κ³ λ¦¬μ¦ μ¤‘ ν•λ‚μΈ PPO(Proximal Policy Optimization)λ¥Ό ν†µν•΄ RollerBall κ²μ„ μ—μ΄μ „νΈλ¥Ό ν•™μµμ‹ν‚¤λ” κ²ƒμ„ λ©ν‘λ΅ ν•©λ‹λ‹¤. μ—μ΄μ „νΈλ” κ³µμ„ μ΅°μΆ…ν•μ—¬ λ§µ μ„μ— μλ” νƒ€κ²μ„ μμ§‘ν•κ³ , λ§µ λ°–μΌλ΅ λ–¨μ–΄μ§€μ§€ μ•λ„λ΅ ν•™μµν•©λ‹λ‹¤.

### κ²μ„ μ†κ°

<img src="./Results/rollerball_introduce.gif" width="500"/>

## π€ κ°λ° ν™κ²½

*   Unity ML-Agents
*   PyTorch
*   Python 3.x

## β™οΈ λ¨λΈ μ•„ν‚¤ν…μ² λ° ν•μ΄νΌνλΌλ―Έν„°

### λ¨λΈ μ•„ν‚¤ν…μ²

ν•™μµμ— μ‚¬μ©λ λ¨λΈμ μ•„ν‚¤ν…μ²λ” μ•„λμ™€ κ°™μµλ‹λ‹¤.

<img src="./Results/architecture.png" width="700"/>

### μƒνƒ λ° ν–‰λ™ κ³µκ°„

*   **μƒνƒ κ³µκ°„ (State Space):** κ³µμ μ„μΉ, νμ „, μ†λ„ λ“± 8κ°μ λ³€μλ΅ κµ¬μ„±λ©λ‹λ‹¤.
*   **ν–‰λ™ κ³µκ°„ (Action Space):** xμ¶•κ³Ό zμ¶• λ°©ν–¥μΌλ΅ κ³µμ— κ°€ν•  νμ„ λ‚νƒ€λ‚΄λ” 2κ°μ μ—°μ†μ μΈ κ°’μΌλ΅ κµ¬μ„±λ©λ‹λ‹¤.

<img src="./Results/state_action.png" width="700"/>

### ν•μ΄νΌνλΌλ―Έν„° νλ‹ κ²°κ³Ό

μµμ μ ν•™μµμ„ μ„ν•΄ ν•μ΄νΌνλΌλ―Έν„° νλ‹μ„ μ§„ν–‰ν–μΌλ©°, κ·Έ κ²°κ³Όλ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤.

<img src="./Results/hyperparameter_tuning.png" width="700"/>

## π“ ν•™μµ κ²°κ³Ό

### λ³΄μƒ λ° μ†μ‹¤ κ·Έλν”„

ν•™μµμ΄ μ§„ν–‰λ¨μ— λ”°λΌ μ—μ΄μ „νΈκ°€ λ°›λ” λ³΄μƒκ³Ό μ†μ‹¤ ν•¨μμ λ³€ν™”λ” μ•„λ κ·Έλν”„μ™€ κ°™μµλ‹λ‹¤.

<img src="Results/reward.png" width="500"/>
<img src="Results/loss_plot.png" width="500"/>

## π¤– ν•™μµλ μ—μ΄μ „νΈ ν”λ μ΄

ν•™μµμ΄ μ™„λ£λ μ—μ΄μ „νΈκ°€ μ‹¤μ λ΅ κ²μ„μ„ ν”λ μ΄ν•λ” λ¨μµμ…λ‹λ‹¤.

<img src="./Results/after_trained_agent.gif" width="500"/>

